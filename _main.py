import asyncio
import time
import streamlit as st

from streamlit_local_storage import LocalStorage

from openai_gpt import gpt_prompt
from google_gemini import gemini_prompt
from anthropic_claude import claude_prompt
from meta_llama import llama_prompt
from qwen_qwen import qwen_prompt

#ì‘ë‹µìš”ì•½ê¸°ëŠ¥ì— ì‚¬ìš©ë˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬
from transformers import pipeline
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from io import BytesIO
import base64

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(layout="wide")


# ë¡œì»¬ ìŠ¤í† ë¦¬ì§€ ì´ˆê¸°í™”
localS = LocalStorage()


# localStorageì— ì €ì¥ëœ promptë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_local_storage():
    prompts = localS.getItem("prompt_history")

    if prompts is None:
        prompts = []
    return prompts


# localStorageì— promptë¥¼ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def set_local_storage(key, value):
    if value:
        prompts = get_local_storage()
        prompts.append(value)
        localS.setItem(key, prompts)


st.markdown(
    """
    <style>
            div.st-emotion-cache-keje6w:nth-child(1) {
                max-width: 360px;
            }

            div.stColumn:nth-child(2) > div:nth-child(1) > div:nth-child(1) > div:nth-child(1) {
                display: flex;
                flex-wrap: wrap;
                flex-direction: row;
                align-items: flex-start;
            }

            div.stColumn:nth-child(2) > div:nth-child(1) > div:nth-child(1) > div:nth-child(1) > .stChatMessage {
                background-color: #F1F2F6;
                border-radius: 8px;
                padding-right: 24px;
                min-width: 400px;
                max-width: 400px;
            }

               div.stColumn:nth-child(2) > div:nth-child(1) > div:nth-child(1) > div:nth-child(1) > .stElementContainer {
                max-width: 120px;
            }

            .st-emotion-cache-1khdzpl > div:nth-child(1) .stColumn {
                background-color: black;
            }
        </style>
    """,
    unsafe_allow_html=True,
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "response_times" not in st.session_state:
    st.session_state["response_times"] = {
        "gpt": [],
        "gemini": [],
        "claude": [],
        "llama": [],
        "qwen": [],
    }

if "prompt_history" not in st.session_state:
    st.session_state["prompt_history"] = []

if "gpt_responses" not in st.session_state:
    st.session_state["gpt_responses"] = []

if "gemini_responses" not in st.session_state:
    st.session_state["gemini_responses"] = []

if "claude_responses" not in st.session_state:
    st.session_state["claude_responses"] = []

if "llama_responses" not in st.session_state:
    st.session_state["llama_responses"] = []

if "qwen_responses" not in st.session_state:
    st.session_state["qwen_responses"] = []

if "ai_display_selection" not in st.session_state:
    st.session_state["ai_display_selection"] = ["ChatGPT", "Gemini", "Claude"]

if "disable_ai_in_tabs" not in st.session_state:
    st.session_state["disable_ai_in_tabs"] = True


# ë¹„ë™ê¸° API í˜¸ì¶œ í•¨ìˆ˜ ì •ì˜
async def fetch_gpt_response(prompt):
    if (
        st.session_state["disable_ai_in_tabs"]
        and "ChatGPT" not in st.session_state["ai_display_selection"]
    ):
        return ""
    start_time = time.time()
    response = await asyncio.to_thread(gpt_prompt, prompt) if prompt else ""
    end_time = time.time()
    st.session_state["response_times"]["gpt"].append(end_time - start_time)
    return response


async def fetch_gemini_response(prompt):
    if (
        st.session_state["disable_ai_in_tabs"]
        and "Gemini" not in st.session_state["ai_display_selection"]
    ):
        return ""
    start_time = time.time()
    response = await asyncio.to_thread(gemini_prompt, prompt) if prompt else ""
    end_time = time.time()
    st.session_state["response_times"]["gemini"].append(end_time - start_time)
    return response


async def fetch_claude_response(prompt):
    if (
        st.session_state["disable_ai_in_tabs"]
        and "Claude" not in st.session_state["ai_display_selection"]
    ):
        return ""
    start_time = time.time()
    response = await asyncio.to_thread(claude_prompt, prompt) if prompt else ""
    end_time = time.time()
    st.session_state["response_times"]["claude"].append(end_time - start_time)
    return response


async def fetch_llama_response(prompt):
    if (
        st.session_state["disable_ai_in_tabs"]
        and "Llama" not in st.session_state["ai_display_selection"]
    ):
        return ""
    start_time = time.time()
    response = await asyncio.to_thread(llama_prompt, prompt) if prompt else ""
    end_time = time.time()
    st.session_state["response_times"]["llama"].append(end_time - start_time)
    return response


async def fetch_qwen_response(prompt):
    if (
        st.session_state["disable_ai_in_tabs"]
        and "Qwen" not in st.session_state["ai_display_selection"]
    ):
        return ""
    start_time = time.time()
    response = await asyncio.to_thread(qwen_prompt, prompt) if prompt else ""
    end_time = time.time()
    st.session_state["response_times"]["qwen"].append(end_time - start_time)
    return response


# ë¹„ë™ê¸° ì²˜ë¦¬ í•¨ìˆ˜
async def fetch_all_responses(prompt):
    responses = await asyncio.gather(
        fetch_gpt_response(prompt),
        fetch_gemini_response(prompt),
        fetch_claude_response(prompt),
        fetch_llama_response(prompt),
        fetch_qwen_response(prompt),
    )
    return responses


# ì´ˆê¸° ì„ íƒ ì˜µì…˜ ì„¤ì •
options = ["ChatGPT", "Gemini", "Claude", "Llama", "Qwen"]
default_selection = ["ChatGPT", "Gemini", "Claude"]

# ìœ ì €ì˜ ìƒˆë¡œìš´ prompt ì…ë ¥
prompt = st.chat_input("í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")

if prompt:
    # ê¸°ì¡´ prompt ê¸°ë¡ì— ìƒˆë¡œìš´ prompt ì¶”ê°€
    st.session_state["prompt_history"].append(prompt)

    # ê° AIì˜ ì‘ë‹µì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ë°›ì•„ì˜¤ê¸°
    responses = asyncio.run(fetch_all_responses(prompt))

    # ê° AIì˜ ì‘ë‹µì„ session_stateì— ê¸°ë¡
    st.session_state["gpt_responses"].append(responses[0])
    st.session_state["gemini_responses"].append(responses[1])
    st.session_state["claude_responses"].append(responses[2])
    st.session_state["llama_responses"].append(responses[3])
    st.session_state["qwen_responses"].append(responses[4])

# íƒ­ êµ¬ì„±
(
    All,
    gpt_as_tab,
    gemini_as_tab,
    claude_as_tab,
    llama_as_tab,
    qwen_as_tab,
    records_as_tab,
    settings_as_tab,
) = st.tabs(["ì „ì²´", "ChatGPT", "Gemini", "Claude", "Llama", "Qwen", "ë¡œê·¸", "ì„¤ì •"])


# íƒ­: Settings
with settings_as_tab:
    st.title("Settings")

    # Pre-select all options by default
    selection = st.pills(
        "ì „ì²´ íƒ­ì—ì„œ í‘œì‹œí•  AIë¥¼ ì„ íƒí•˜ì„¸ìš”.",
        options,
        default=default_selection,
        selection_mode="multi",
    )

    # Store the selection in session state for persistence
    st.session_state["ai_display_selection"] = selection

    # Add the option to disable AI in individual tabs
    st.session_state["disable_ai_in_tabs"] = st.checkbox(
        "ë¹„í™œì„±í™”í•œ AI ëª¨ë¸ì„ í”„ë¡œê·¸ë¨ ì „ì²´ì—ì„œ ë¹„í™œì„±í™”í•˜ì—¬ ì‘ë‹µ ì†ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.",
        value=True,
    )


# íƒ­: ê¸°ë¡
with records_as_tab:
    stored_prompts = localS.getItem("prompt_history")
    st.markdown("#### ë¡œê·¸ë¥¼ í™•ì¸í•˜ì‹œë ¤ë©´ ìƒˆë¡œê³ ì¹¨ í•´ ì£¼ì„¸ìš”!")

    def delete_prompt_history():
        localS.deleteItem("prompt_history")

    if stored_prompts:
        st.button("ë¡œê·¸ ì „ì²´ ì‚­ì œ", type="primary", on_click=delete_prompt_history)

    if stored_prompts:
        for result in stored_prompts:
            with st.chat_message("user"):
                if result[1]["prompt"]:
                    st.write(result[1]["prompt"])
            with st.chat_message("ai", avatar="./assets/gpt.svg"):
                st.markdown(
                    f"""
             <p>
                 {result[1]["gpt_response"].replace("['", "").replace("']", "").replace("\\n", "<br />").replace("`", "")}
             </p>
             """,
                    unsafe_allow_html=True,
                )
            with st.chat_message("ai", avatar="./assets/gemini.svg"):
                st.markdown(
                    f"""
             <p>
                 {result[1]["gemini_response"].replace("['", "").replace("']", "").replace("\\n", "<br />").replace("`", "")}
             </p>
             """,
                    unsafe_allow_html=True,
                )
            with st.chat_message("ai", avatar="./assets/claude.svg"):
                st.markdown(
                    f"""
             <p>
                 {result[1]["claude_response"].replace("['", "").replace("']", "").replace("\\n", "<br />").strip("[]").strip('""').replace("`", "")}
             </p>
             """,
                    unsafe_allow_html=True,
                )
            with st.chat_message("ai", avatar="./assets/meta.png"):
                st.markdown(
                    f"""
             <p>
                 {result[1]["llama_response"].replace("['", "").replace("']", "").replace("\\n", "<br />").replace("`", "")}
             </p>
             """,
                    unsafe_allow_html=True,
                )
            with st.chat_message("ai", avatar="./assets/qwen.png"):
                st.markdown(
                    f"""
             <p>
                 {result[1]["qwen_response"].replace("['", "").replace("']", "").replace("\\n", "<br />").replace("`", "")}
             </p>
             """,
                    unsafe_allow_html=True,
                )


# íƒ­: ì „ì²´
with All:
    input_as_col, output_as_col = st.columns(2)

    with input_as_col:
        if not prompt:
            with st.chat_message("user"):
                st.markdown("**USER**")

        # ì´ì „ ì…ë ¥ë„ í¬í•¨í•˜ì—¬ ë³´ì—¬ì£¼ê¸°
        for prompt_text in st.session_state["prompt_history"]:
            if not isinstance(prompt_text, dict):
                with st.chat_message("user"):
                    st.write(prompt_text)

    with output_as_col:
        # Retrieve the selection from session state, defaulting to all options if not set
        display_selection = st.session_state.get(
            "ai_display_selection", default_selection
        )

        ai_configs = [
            {
                "name": "ChatGPT",
                "responses": st.session_state["gpt_responses"],
                "times": st.session_state["response_times"]["gpt"],
                "avatar": "./assets/gpt.svg",
                "model": "OpenAI: gpt-4o-mini",
            },
            {
                "name": "Gemini",
                "responses": st.session_state["gemini_responses"],
                "times": st.session_state["response_times"]["gemini"],
                "avatar": "./assets/gemini.svg",
                "model": "Google: Gemini-1.5-flash",
            },
            {
                "name": "Claude",
                "responses": st.session_state["claude_responses"],
                "times": st.session_state["response_times"]["claude"],
                "avatar": "./assets/claude.svg",
                "model": "Anthropic: Claude-3.5-Sonnet",
            },
            {
                "name": "Llama",
                "responses": st.session_state["llama_responses"],
                "times": st.session_state["response_times"]["llama"],
                "avatar": "./assets/meta.png",
                "model": "Meta: Llama-3.2-90B-Vision-Instruct-Turbo",
            },
            {
                "name": "Qwen",
                "responses": st.session_state["qwen_responses"],
                "times": st.session_state["response_times"]["qwen"],
                "avatar": "./assets/qwen.png",
                "model": "Qwen: Qwen2.5-72B-Instruct-Turbo",
            },
        ]

        for config in ai_configs:
            if config["name"] in display_selection:
                with st.chat_message("ai", avatar=config["avatar"]):
                    if not prompt:
                        st.markdown(f"**{config['model']}**")
                    else:
                        # ì‘ë‹µê³¼ í•´ë‹¹ ì‘ë‹µì˜ ì‹œê°„ì„ í•¨ê»˜ ê²°í•©
                        responses_with_times = []
                        for response, time in zip(config["responses"], config["times"]):
                            responses_with_times.append(
                                f"{response}\n\nì‘ë‹µ ì‹œê°„: {time:.2f} ì´ˆ\n\n---\n\n"
                            )

                        # ê²°í•©ëœ ì‘ë‹µ í‘œì‹œ (ì‘ë‹µ ì‚¬ì´ ê°„ê²© ë„“í˜)
                        st.markdown("\n\n".join(responses_with_times))

# íƒ­: GPT
with gpt_as_tab:
    if "ChatGPT" in st.session_state["ai_display_selection"]:
        st.title("ğŸ’¬ OpenAI: gpt-4o-mini")

        # 'prompt_history'ì™€ 'gpt_responses'ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if "prompt_history" in st.session_state and "gpt_responses" in st.session_state:
            prompt_history = st.session_state["prompt_history"]
            gpt_responses = st.session_state["gpt_responses"]

            # ëŒ€í™” ê¸°ë¡ì„ ìˆœì°¨ì ìœ¼ë¡œ í‘œì‹œ
            for i in range(len(prompt_history)):
                # ìœ ì €ì˜ í”„ë¡¬í”„íŠ¸ í‘œì‹œ (ìœ ì € ë©”ì‹œì§€ê°€ ë¨¼ì €)
                prompt_text = prompt_history[i]
                if prompt_text and not isinstance(prompt_text, dict):
                    with st.chat_message("user"):
                        st.write(prompt_text)

                # AIì˜ ì‘ë‹µ í‘œì‹œ (AI ì‘ë‹µì´ ê·¸ ë’¤ì—)
                if i < len(gpt_responses):
                    response = gpt_responses[i]
                    with st.chat_message("ai", avatar="./assets/gpt.svg"):
                        st.write(response)

        else:
            st.write("ëŒ€í™” ë‚´ì—­ì´ ì—†ìŠµë‹ˆë‹¤.")

        # ì‘ë‹µ ì‹œê°„ ì¶œë ¥
        if st.session_state["response_times"]["gpt"]:
            st.write(
                f"ì‘ë‹µ ì‹œê°„: {sum(st.session_state['response_times']['gpt']):.2f} ì´ˆ"
            )
    else:
        st.markdown("# ~~ğŸ’¬ OpenAI: gpt-4o-mini~~")
        st.write(
            "í•´ë‹¹ AI ëª¨ë¸ì€ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ì„¤ì • íƒ­ì—ì„œ í™œì„±í™” í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )


# íƒ­: Gemini
with gemini_as_tab:
    if "Gemini" in st.session_state["ai_display_selection"]:
        st.title("ğŸ’¬ Google: Gemini-1.5-flash")

        # 'prompt_history'ì™€ 'gemini_responses'ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if (
            "prompt_history" in st.session_state
            and "gemini_responses" in st.session_state
        ):
            prompt_history = st.session_state["prompt_history"]
            gemini_responses = st.session_state["gemini_responses"]

            # ëŒ€í™” ê¸°ë¡ì„ ìˆœì°¨ì ìœ¼ë¡œ í‘œì‹œ
            for i in range(len(prompt_history)):
                # ìœ ì €ì˜ í”„ë¡¬í”„íŠ¸ í‘œì‹œ (ìœ ì € ë©”ì‹œì§€ê°€ ë¨¼ì €)
                prompt_text = prompt_history[i]
                if prompt_text and not isinstance(prompt_text, dict):
                    with st.chat_message("user"):
                        st.write(prompt_text)

                # AIì˜ ì‘ë‹µ í‘œì‹œ (AI ì‘ë‹µì´ ê·¸ ë’¤ì—)
                if i < len(gemini_responses):
                    response = gemini_responses[i]
                    with st.chat_message("ai", avatar="./assets/gemini.svg"):
                        st.write(response)

        else:
            st.write("ëŒ€í™” ë‚´ì—­ì´ ì—†ìŠµë‹ˆë‹¤.")

        # ì‘ë‹µ ì‹œê°„ ì¶œë ¥
        if st.session_state["response_times"]["gemini"]:
            st.write(
                f"ì‘ë‹µ ì‹œê°„: {sum(st.session_state['response_times']['gemini']):.2f} ì´ˆ"
            )
    else:
        st.markdown("# ~~ğŸ’¬ Google: Gemini-1.5-flash~~")
        st.write(
            "í•´ë‹¹ AI ëª¨ë¸ì€ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ì„¤ì • íƒ­ì—ì„œ í™œì„±í™” í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )


# íƒ­: Claude
with claude_as_tab:
    if "Claude" in st.session_state["ai_display_selection"]:
        st.title("ğŸ’¬ Anthropic: Claude-3.5-Sonnet")

        # 'prompt_history'ì™€ 'claude_responses'ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if (
            "prompt_history" in st.session_state
            and "claude_responses" in st.session_state
        ):
            prompt_history = st.session_state["prompt_history"]
            claude_responses = st.session_state["claude_responses"]

            # ëŒ€í™” ê¸°ë¡ì„ ìˆœì°¨ì ìœ¼ë¡œ í‘œì‹œ
            for i in range(len(prompt_history)):
                # ìœ ì €ì˜ í”„ë¡¬í”„íŠ¸ í‘œì‹œ (ìœ ì € ë©”ì‹œì§€ê°€ ë¨¼ì €)
                prompt_text = prompt_history[i]
                if prompt_text and not isinstance(prompt_text, dict):
                    with st.chat_message("user"):
                        st.write(prompt_text)

                # AIì˜ ì‘ë‹µ í‘œì‹œ (AI ì‘ë‹µì´ ê·¸ ë’¤ì—)
                if i < len(claude_responses):
                    response = claude_responses[i]
                    with st.chat_message("ai", avatar="./assets/claude.svg"):
                        st.write(response)

        else:
            st.write("ëŒ€í™” ë‚´ì—­ì´ ì—†ìŠµë‹ˆë‹¤.")

        # ì‘ë‹µ ì‹œê°„ ì¶œë ¥
        if st.session_state["response_times"]["claude"]:
            st.write(
                f"ì‘ë‹µ ì‹œê°„: {sum(st.session_state['response_times']['claude']):.2f} ì´ˆ"
            )
    else:
        st.markdown("# ~~ğŸ’¬ Anthropic: Claude-3.5-Sonnet~~")
        st.write(
            "í•´ë‹¹ AI ëª¨ë¸ì€ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ì„¤ì • íƒ­ì—ì„œ í™œì„±í™” í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )


# íƒ­: Llama
with llama_as_tab:
    if "Llama" in st.session_state["ai_display_selection"]:
        st.title("ğŸ’¬ Meta: Llama-3.2-90B-Vision-Instruct-Turbo")

        # 'prompt_history'ì™€ 'llama_responses'ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if (
            "prompt_history" in st.session_state
            and "llama_responses" in st.session_state
        ):
            prompt_history = st.session_state["prompt_history"]
            llama_responses = st.session_state["llama_responses"]

            # ëŒ€í™” ê¸°ë¡ì„ ìˆœì°¨ì ìœ¼ë¡œ í‘œì‹œ
            for i in range(len(prompt_history)):
                # ìœ ì €ì˜ í”„ë¡¬í”„íŠ¸ í‘œì‹œ (ìœ ì € ë©”ì‹œì§€ê°€ ë¨¼ì €)
                prompt_text = prompt_history[i]
                if prompt_text and not isinstance(prompt_text, dict):
                    with st.chat_message("user"):
                        st.write(prompt_text)

                # AIì˜ ì‘ë‹µ í‘œì‹œ (AI ì‘ë‹µì´ ê·¸ ë’¤ì—)
                if i < len(llama_responses):
                    response = llama_responses[i]
                    with st.chat_message("ai", avatar="./assets/meta.png"):
                        st.write(response)

        else:
            st.write("ëŒ€í™” ë‚´ì—­ì´ ì—†ìŠµë‹ˆë‹¤.")

        # ì‘ë‹µ ì‹œê°„ ì¶œë ¥
        if st.session_state["response_times"]["llama"]:
            st.write(
                f"ì‘ë‹µ ì‹œê°„: {sum(st.session_state['response_times']['llama']):.2f} ì´ˆ"
            )
    else:
        st.markdown("# ~~ğŸ’¬ Meta: Llama-3.2-90B-Vision-Instruct-Turbo~~")
        st.write(
            "í•´ë‹¹ AI ëª¨ë¸ì€ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ì„¤ì • íƒ­ì—ì„œ í™œì„±í™” í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )


# íƒ­: Qwen
with qwen_as_tab:
    if "Qwen" in st.session_state["ai_display_selection"]:
        st.title("ğŸ’¬ Qwen: Qwen2.5-72B-Instruct-Turbo")

        # 'prompt_history'ì™€ 'qwen_responses'ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if (
            "prompt_history" in st.session_state
            and "qwen_responses" in st.session_state
        ):
            prompt_history = st.session_state["prompt_history"]
            qwen_responses = st.session_state["qwen_responses"]

            # ëŒ€í™” ê¸°ë¡ì„ ìˆœì°¨ì ìœ¼ë¡œ í‘œì‹œ
            for i in range(len(prompt_history)):
                # ìœ ì €ì˜ í”„ë¡¬í”„íŠ¸ í‘œì‹œ (ìœ ì € ë©”ì‹œì§€ê°€ ë¨¼ì €)
                prompt_text = prompt_history[i]
                if prompt_text and not isinstance(prompt_text, dict):
                    with st.chat_message("user"):
                        st.write(prompt_text)

                # AIì˜ ì‘ë‹µ í‘œì‹œ (AI ì‘ë‹µì´ ê·¸ ë’¤ì—)
                if i < len(qwen_responses):
                    response = qwen_responses[i]
                    with st.chat_message("ai", avatar="./assets/qwen.png"):
                        st.write(response)

        else:
            st.write("ëŒ€í™” ë‚´ì—­ì´ ì—†ìŠµë‹ˆë‹¤.")

        # ì‘ë‹µ ì‹œê°„ ì¶œë ¥
        if st.session_state["response_times"]["qwen"]:
            st.write(
                f"ì‘ë‹µ ì‹œê°„: {sum(st.session_state['response_times']['qwen']):.2f} ì´ˆ"
            )
    else:
        st.markdown("# ~~ğŸ’¬ Qwen: Qwen2.5-72B-Instruct-Turbo~~")
        st.write(
            "í•´ë‹¹ AI ëª¨ë¸ì€ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ì„¤ì • íƒ­ì—ì„œ í™œì„±í™” í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )


# 'prompt_history'ê°€ ì„¸ì…˜ì— ì—†ìœ¼ë©´ ì´ˆê¸°í™”
if "prompt_history" not in st.session_state:
    st.session_state["prompt_history"] = []

if prompt:
    st.session_state["prompt_history"].append(
        {
            "prompt": prompt,
            "gpt_response": str(st.session_state.get("gpt_responses", "")),
            "gemini_response": str(st.session_state.get("gemini_responses", "")),
            "claude_response": str(st.session_state.get("claude_responses", "")),
            "llama_response": str(st.session_state.get("llama_responses", "")),
            "qwen_response": str(st.session_state.get("qwen_responses", "")),
        }
    )

# ë¡œì»¬ ìŠ¤í† ë¦¬ì§€ì— ì €ì¥
set_local_storage("prompt_history", st.session_state["prompt_history"])



###############ì¢…í˜„ê¸°ëŠ¥ì¶”ê°€##

# ìš”ì•½ ëª¨ë¸ ì´ˆê¸°í™”
summarizer = pipeline("summarization")

def summarize_text(text, max_length=60, min_length=25):
    """
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜
    :param text: ìš”ì•½í•  í…ìŠ¤íŠ¸
    :param max_length: ìš”ì•½ë¬¸ ìµœëŒ€ ê¸¸ì´
    :param min_length: ìš”ì•½ë¬¸ ìµœì†Œ ê¸¸ì´
    :return: ìš”ì•½ëœ í…ìŠ¤íŠ¸
    """
    if not text or len(text) < min_length:
        return text  # í…ìŠ¤íŠ¸ê°€ ì§§ìœ¼ë©´ ìš”ì•½í•˜ì§€ ì•ŠìŒ
    summary = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)
    return summary[0]['summary_text']

def generate_wordcloud(text):
    """
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ WordCloud ìƒì„±
    :param text: WordCloudë¥¼ ìƒì„±í•  í…ìŠ¤íŠ¸
    :return: WordCloud ì´ë¯¸ì§€ì˜ base64 ì¸ì½”ë”© ê°’
    """
    if not text:
        return None

    # WordCloud ìƒì„±
    wordcloud = WordCloud(
        width=800,
        height=400,
        background_color='white',
        colormap='viridis',
        max_words=100
    ).generate(text)

    # WordCloud ì´ë¯¸ì§€ë¥¼ BytesIOì— ì €ì¥
    image_stream = BytesIO()
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.tight_layout(pad=0)
    plt.savefig(image_stream, format='png')
    plt.close()

    # base64ë¡œ ì¸ì½”ë”©
    image_stream.seek(0)
    return base64.b64encode(image_stream.getvalue()).decode('utf-8')

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
for key in ["gpt", "gemini", "claude", "llama", "qwen"]:
    if f"{key}_summaries" not in st.session_state:
        st.session_state[f"{key}_summaries"] = []
    if f"{key}_wordclouds" not in st.session_state:
        st.session_state[f"{key}_wordclouds"] = []

# ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ì‹œ ìš”ì•½ ë° WordCloud ìƒì„±
if prompt:
    summaries = []
    wordclouds = []

    for response in responses:  # AI ëª¨ë¸ ì‘ë‹µ ë¦¬ìŠ¤íŠ¸
        summaries.append(summarize_text(response))
        wordclouds.append(generate_wordcloud(response))

    # ê° ëª¨ë¸ì˜ ìš”ì•½ ë° WordCloud ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
    st.session_state["gpt_summaries"].append(summaries[0])
    st.session_state["gemini_summaries"].append(summaries[1])
    st.session_state["claude_summaries"].append(summaries[2])
    st.session_state["llama_summaries"].append(summaries[3])
    st.session_state["qwen_summaries"].append(summaries[4])

    st.session_state["gpt_wordclouds"].append(wordclouds[0])
    st.session_state["gemini_wordclouds"].append(wordclouds[1])
    st.session_state["claude_wordclouds"].append(wordclouds[2])
    st.session_state["llama_wordclouds"].append(wordclouds[3])
    st.session_state["qwen_wordclouds"].append(wordclouds[4])

# íƒ­ êµ¬ì„±ì— ìš”ì•½ ë° WordCloud ì¶”ê°€
with All:
    st.write("### ì‘ë‹µ ìš”ì•½ ë° WordCloud")

    for i, (model_name, summaries, wordclouds) in enumerate([
        ("GPT", st.session_state["gpt_summaries"], st.session_state["gpt_wordclouds"]),
        ("Gemini", st.session_state["gemini_summaries"], st.session_state["gemini_wordclouds"]),
        ("Claude", st.session_state["claude_summaries"], st.session_state["claude_wordclouds"]),
        ("Llama", st.session_state["llama_summaries"], st.session_state["llama_wordclouds"]),
        ("Qwen", st.session_state["qwen_summaries"], st.session_state["qwen_wordclouds"]),
    ]):
        st.markdown(f"**{model_name} ìš”ì•½**:")
        st.write(summaries[-1] if summaries else "ìš”ì•½ ì—†ìŒ")

        st.markdown(f"**{model_name} WordCloud**:")
        if wordclouds and wordclouds[-1]:
            st.image(f"data:image/png;base64,{wordclouds[-1]}", use_column_width=True)
        else:
            st.write("WordCloud ìƒì„± ì‹¤íŒ¨")
