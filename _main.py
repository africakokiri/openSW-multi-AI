import streamlit as st


# í˜ì´ì§€ ì„¤ì •
st.set_page_config(layout="wide")


# íƒ­ êµ¬ì„±
All, gpt_as_tab, gemini_as_tab, claude_as_tab, llama_as_tab = st.tabs(
    ["ì „ì²´", "chatGPT", "Gemini", "Claude", "llama"]
)

# íƒ­: ì „ì²´
with All:
    gpt_as_col, gemini_as_col, claude_as_col, llama_as_col = st.columns(4)

    with gpt_as_col:
        st.write("GPT")

    with gemini_as_col:
        st.write("Gemini")

    with claude_as_col:
        st.write("Claude")

    with llama_as_col:
        st.write("LlaMA")


# íƒ­: chatGPT
with gpt_as_tab:
    st.title("ğŸ’¬ openAI: GPT-4o-mini")
    st.caption("ğŸš€ A Streamlit chatbot powered by openAI ChatGPT")


# íƒ­: Gemini
with gemini_as_tab:
    st.title("ğŸ’¬ Google: Gemini-1.5-flash")
    st.caption("ğŸš€ A Streamlit chatbot powered by Google Gemini")


# íƒ­: Claude
with claude_as_tab:
    st.title("ğŸ’¬ Anthropic: claude-3-5-sonnet")
    st.caption("ğŸš€ A Streamlit chatbot powered by Anthropic Claude")


# íƒ­: llama
with llama_as_tab:
    st.title("ğŸ’¬ Meta: llama3.2-90b-vision")
    st.caption("ğŸš€ A Streamlit chatbot powered by Meta LlaMA")


# ìœ ì €ì˜ promptë¥¼ stateì— ì €ì¥
prompt = st.chat_input("í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
st.session_state["prompt"] = prompt

import asyncio
import time
import streamlit as st
from openai_gpt import gpt_prompt
from google_gemini import gemini_prompt
from anthropic_claude import claude_prompt
from meta_llama import llama_prompt

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(layout="wide")

st.markdown(
    """
        <style>
            div.st-emotion-cache-keje6w:nth-child(1) {
                max-width: 360px;
            }
            
            .st-emotion-cache-14hz42n > div:nth-child(1) {
                display: flex;
                flex-wrap: wrap;
            }
            
            div.st-emotion-cache-12w0qpk:nth-child(1), div.st-emotion-cache-12w0qpk:nth-child(2), div.st-emotion-cache-12w0qpk:nth-child(3), div.st-emotion-cache-12w0qpk:nth-child(4){
                min-width: 400px;
                max-width: 400px;
            }
            
            .st-emotion-cache-1khdzpl > div:nth-child(1) .stColumn {
                background-color: black;
            }
            
            div.st-emotion-cache-12w0qpk:nth-child(1), div.st-emotion-cache-12w0qpk:nth-child(2), div.st-emotion-cache-12w0qpk:nth-child(3), div.st-emotion-cache-12w0qpk:nth-child(4) {
                background-color: #F1F2F6;
                border-radius: 8px;
                padding-right: 24px;
            }
        </style>
    """,
    unsafe_allow_html=True,
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "response_times" not in st.session_state:
    st.session_state["response_times"] = {
        "gpt": [],
        "gemini": [],
        "claude": [],
        "llama": [],
    }

if "prompt_history" not in st.session_state:
    st.session_state["prompt_history"] = []

if "gpt_responses" not in st.session_state:
    st.session_state["gpt_responses"] = []

if "gemini_responses" not in st.session_state:
    st.session_state["gemini_responses"] = []

if "claude_responses" not in st.session_state:
    st.session_state["claude_responses"] = []

if "llama_responses" not in st.session_state:
    st.session_state["llama_responses"] = []


# ë¹„ë™ê¸° API í˜¸ì¶œ í•¨ìˆ˜ ì •ì˜
async def fetch_gpt_response(prompt):
    start_time = time.time()
    response = gpt_prompt(prompt) if prompt else ""
    end_time = time.time()
    st.session_state["response_times"]["gpt"].append(end_time - start_time)
    return response


async def fetch_gemini_response(prompt):
    start_time = time.time()
    response = gemini_prompt(prompt) if prompt else ""
    end_time = time.time()
    st.session_state["response_times"]["gemini"].append(end_time - start_time)
    return response


async def fetch_claude_response(prompt):
    start_time = time.time()
    response = claude_prompt(prompt) if prompt else ""
    end_time = time.time()
    st.session_state["response_times"]["claude"].append(end_time - start_time)
    return response


async def fetch_llama_response(prompt):
    start_time = time.time()
    response = llama_prompt(prompt) if prompt else ""
    end_time = time.time()
    st.session_state["response_times"]["llama"].append(end_time - start_time)
    return response


# ë¹„ë™ê¸° ì²˜ë¦¬ í•¨ìˆ˜
async def fetch_all_responses(prompt):
    responses = await asyncio.gather(
        fetch_gpt_response(prompt),
        fetch_gemini_response(prompt),
        fetch_claude_response(prompt),
        fetch_llama_response(prompt),
    )
    return responses


# ìœ ì €ì˜ ìƒˆë¡œìš´ prompt ì…ë ¥
prompt = st.chat_input("í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")

if prompt:
    # ê¸°ì¡´ prompt ê¸°ë¡ì— ìƒˆë¡œìš´ prompt ì¶”ê°€
    st.session_state["prompt_history"].append(prompt)

    # ê° AIì˜ ì‘ë‹µì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ë°›ì•„ì˜¤ê¸°
    responses = asyncio.run(fetch_all_responses(prompt))

    # ê° AIì˜ ì‘ë‹µì„ session_stateì— ê¸°ë¡
    st.session_state["gpt_responses"].append(responses[0])
    st.session_state["gemini_responses"].append(responses[1])
    st.session_state["claude_responses"].append(responses[2])
    st.session_state["llama_responses"].append(responses[3])

# íƒ­ êµ¬ì„±
All, gpt_as_tab, gemini_as_tab, claude_as_tab, llama_as_tab, settings = st.tabs(
    ["ì „ì²´", "chatGPT", "Gemini", "Claude", "llama", "ì„¤ì •"]
)

# íƒ­: ì „ì²´
with All:
    input_as_col, output_as_col = st.columns(2)

    with input_as_col:
        if not prompt:
            with st.chat_message("user"):
                st.markdown("**USER**")

        # ì´ì „ ì…ë ¥ë„ í¬í•¨í•˜ì—¬ ë³´ì—¬ì£¼ê¸°
        for prompt_text in st.session_state["prompt_history"]:
            with st.chat_message("user"):
                st.write(prompt_text)

    with output_as_col:
        gpt_as_col, gemini_as_col, claude_as_col, llama_as_col = st.columns(4)

        with gpt_as_col:
            if not prompt:
                with st.chat_message("ai", avatar="./assets/gpt.svg"):
                    st.markdown("**openAI: gpt-4o-mini**")

            if prompt:
                # ì´ì „ ì‘ë‹µë„ í¬í•¨í•˜ì—¬ ë³´ì—¬ì£¼ê¸°
                for response in st.session_state["gpt_responses"]:
                    with st.chat_message("ai", avatar="./assets/gpt.svg"):
                        st.write(response)
                # ì‘ë‹µ ì‹œê°„ í‘œì‹œ
                st.write(
                    f"ì‘ë‹µ ì‹œê°„: {sum(st.session_state['response_times']['gpt']):.2f} ì´ˆ"
                )

        with gemini_as_col:
            if not prompt:
                with st.chat_message("ai", avatar="./assets/gemini.svg"):
                    st.markdown("**Google: Gemini-1.5-flash**")

            if prompt:
                # ì´ì „ ì‘ë‹µë„ í¬í•¨í•˜ì—¬ ë³´ì—¬ì£¼ê¸°
                for response in st.session_state["gemini_responses"]:
                    with st.chat_message("ai", avatar="./assets/gemini.svg"):
                        st.write(response)
                # ì‘ë‹µ ì‹œê°„ í‘œì‹œ
                st.write(
                    f"ì‘ë‹µ ì‹œê°„: {sum(st.session_state['response_times']['gemini']):.2f} ì´ˆ"
                )

        with claude_as_col:
            if not prompt:
                with st.chat_message("ai", avatar="./assets/claude.svg"):
                    st.markdown("**Anthropic: Claude-3-5-sonnet**")

            if prompt:
                # ì´ì „ ì‘ë‹µë„ í¬í•¨í•˜ì—¬ ë³´ì—¬ì£¼ê¸°
                for response in st.session_state["claude_responses"]:
                    with st.chat_message("ai", avatar="./assets/claude.svg"):
                        st.write(response)
                # ì‘ë‹µ ì‹œê°„ í‘œì‹œ
                st.write(
                    f"ì‘ë‹µ ì‹œê°„: {sum(st.session_state['response_times']['claude']):.2f} ì´ˆ"
                )

        with llama_as_col:
            if not prompt:
                with st.chat_message("ai", avatar="./assets/meta.png"):
                    st.markdown("**Meta: Llama-3.2-90B-Vision-Instruct-Turbo**")

            if prompt:
                # ì´ì „ ì‘ë‹µë„ í¬í•¨í•˜ì—¬ ë³´ì—¬ì£¼ê¸°
                for response in st.session_state["llama_responses"]:
                    with st.chat_message("ai", avatar="./assets/meta.png"):
                        st.write(response)
                # ì‘ë‹µ ì‹œê°„ í‘œì‹œ
                st.write(
                    f"ì‘ë‹µ ì‹œê°„: {sum(st.session_state['response_times']['llama']):.2f} ì´ˆ"
                )

# ë‚˜ë¨¸ì§€ íƒ­ êµ¬ì„± (ê°œë³„ íƒ­ì— ëŒ€í•´ì„œë„ ì‘ë‹µ ì‹œê°„ í‘œì‹œ ì¶”ê°€ ê°€ëŠ¥)
# ì˜ˆì‹œ: íƒ­ì—ì„œ ì‘ë‹µ ì‹œê°„ í‘œì‹œ
with gpt_as_tab:
    st.title("ğŸ’¬ openAI: gpt-4o-mini")
    st.caption("ğŸš€ A Streamlit chatbot powered by openAI ChatGPT")
    for response in st.session_state["gpt_responses"]:
        with st.chat_message("user"):
            st.write(prompt)
        with st.chat_message("ai", avatar="./assets/gpt.svg"):
            st.write(response)
    st.write(f"ì‘ë‹µ ì‹œê°„: {sum(st.session_state['response_times']['gpt']):.2f} ì´ˆ")

# ê°™ì€ ë°©ì‹ìœ¼ë¡œ ë‹¤ë¥¸ íƒ­ì—ì„œë„ ì‘ë‹µ ì‹œê°„ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
